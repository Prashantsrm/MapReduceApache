from pyspark.sql import SparkSession
from pyspark.sql.functions import col, input_file_name

spark = SparkSession.builder.appName("BookMetadata").getOrCreate()

books_df = spark.read.text("/home/hp/books/D184MB/*.txt") \
    .withColumn("file_name", input_file_name()) \
    .withColumnRenamed("value", "text")

books_df.show(2, truncate=False)


from pyspark.sql.functions import regexp_extract

books_df = books_df.withColumn(
    "title",
    regexp_extract(col("text"), r"Title:\s*(.*)", 1)
)


books_df = books_df.withColumn(
    "release_date",
    regexp_extract(col("text"), r"Release Date:\s*(.*)", 1)
)

books_df = books_df.withColumn(
    "language",
    regexp_extract(col("text"), r"Language:\s*(.*)", 1)
)

books_df = books_df.withColumn(
    "encoding",
    regexp_extract(col("text"), r"Encoding:\s*(.*)", 1)
)

metadata_df = books_df.filter(col("title") != "")
metadata_df.select("file_name", "title", "release_date", "language", "encoding").show(5, truncate=False)

from pyspark.sql.functions import regexp_extract

metadata_df = metadata_df.withColumn(
    "release_year",
    regexp_extract(col("release_date"), r"(\d{4})", 1)
)

metadata_df.groupBy("release_year") \
    .count() \
    .orderBy("release_year") \
    .show()


metadata_df.groupBy("language") \
    .count() \
    .orderBy(col("count").desc()) \
    .show(1)


from pyspark.sql.functions import length, avg

metadata_df.select(avg(length(col("title")))).show()
